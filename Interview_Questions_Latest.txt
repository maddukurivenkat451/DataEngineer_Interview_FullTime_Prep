******************************************
*********NIKE *** NIKE *** NIKE**************
*********************************************

###Team 1 =  (Under Zubair Director team questions)
1. Describe your approach to optimizing Spark applications for performance. 
2. Share examples of real-time data pipeline production issues you've encountered and resolved. 
3. Explain your strategies for handling data skewness in distributed data processing. 
4. Discuss suitable AWS storage options for PySpark output and the factors influencing your choice. 
5. Explain when and why you would use AWS Step Functions to orchestrate PySpark jobs. 
6. Describe how you would manage and deploy PySpark job dependencies within an AWS workflow. 
7. Explain how PySpark transformations can lead to memory issues and how you've addressed them. 
10.How could you enhance an employee information project using PySpark and AWS services? 
11.Explain how to create and configure an Amazon S3 bucket. 
12.Describe the different S3 storage classes and how to manage object lifecycles and retention. 
13.Explain how to combine data from multiple files in PySpark. 
14.Describe your methods for handling and replacing null values in data. 
17.What version control systems and platforms do you use for code management? 
18.Explain the performance advantages and use cases of using a broadcast join in Spark.
22. Explain SCD1,SCD2 & SCD3 types in Datawarehouse. Which one used when developing in your ETL piepline?

23. Explain SCD2 sql logic? How do you implement?
23. Explain Windows Analytical functions in hive by taking an example dataset.

24. How to choose number of executors and memory allocated to executors in spark?

25. Explain CI/CD Process you are using for you Pipeline
26. Design a data pipeline to load data from an API into a data warehouse.  
27. How would you identify and fix data discrepancies in a pipeline?  
28. Explain how you would scale a pipeline to handle a sudden increase in data volume? 
29. Describe a situation where you had to troubleshoot a failed data pipeline?
30. How would you implement a fault-tolerant system for data ingestion?

15.Write an SQL query to find cities with customers from multiple states and count customers per city. 
16.Write an SQL query to distribute comments by user count for users joining Facebook between 2018-2020 in January 2020.

8. Describe your experience using Apache Airflow for data pipeline orchestration on AWS. 
19. Explain the different operators used in airflow to orchestrate the etl jobs?
20. If you need to trigger dependency airflow dag which operator you use and explain the steps?
21. Airflow 1.x vs 2.x difference and which airflow version you are using in production?

9. Explain the purpose and key functionalities of snowflake Data warehouse in your project. 
31. Explain snowflake types of warehouses and its usage ?
32. How to Handle Failed Tasks in Snowflake’s Task? Explain me the process.


###Team 2 : 
1. How do you optimize a slow query on a table with billions of rows?
3. What is the difference between a hash join, merge join, and nested loop join? When would you use each?
4. How would you design a database schema to store log records from distributed systems with queries optimized for time-based analysis?
5. How do you manage indexing on a table where frequent INSERTS, UPDATES, and QUERIES occur simultaneously?
6. Explain the difference between sharding and partitioning in databases. Which one is better for scaling?


8. How would you design an SQL query to detect gaps in sequential IDs?
9. When working with time-series data, what strategies do you use to optimize performance and reduce storage
10. How do you handle incremental data updates in an ETL pipeline?
11. Design an ETL pipeline that cleanses and aggregates terabytes of user activity logs efficiently.
12. Explain the concept of late-arriving dimensions in a data warehouse. How would you handle it?
13. What is the role of Change Data Capture CDC in ETL, and how would you implement it for a high-volume system?
14. How do you deduplicate millions of records in an ETL pipeline without using SQL?
15. Compare batch processing vs streaming processing in an ETL system. When is each approach ideal?
16. What are the common challenges when performing ETL on JSON or semi-structured data, and how do you address them?
17. How do you ensure data consistency and integrity in a distributed ETL pipeline?
18. How would you design an ETL pipeline for real-time fraud detection using tools like Apache Kafka and Apache Spark?
19. Compare Apache Spark and Apache Flink for real-time data processing. Which is better for low-latency systems?
20. Explain how HDFS replication works and how you ensure data durability in case of node failures.
21. Design an ETL pipeline with AWS focused tools for Batch and streaming? Take an Example (Hint = RDS, S3, Glue, EMR, Athena, Redshift ) Streaming (Kinesis, s3, lambda, Athena, Redshift)
22. How do you approach if an use case give to build etl pipeline and what are the steps you will be taken?

23. Explain CTEs vs Subqueries vs Temp Tables vs Views in SQL?
26. What is CI/CD process you are following? Explain the steps to deploy code to prod environment.
27. What is YAML file and how it is used to deploy CI/CD process?
26. When you ingest data from external API or RDBMS systems in to your S3. Suppose few records didn't get ingested how would you handle those scenarios?


24. What was your experience with Airflow buling ETL pipelines? Explain different operators you used in your pipeline?

25. What was your experience working with snowflake? How it is different from data lakehouse?

2. Write an SQL query to calculate a running total for a sales table partitioned by regions.
7. Write a query to pivot rows to columns in a large dataset.

###Team 3 : 

1 What is Broadcasting in PySpark, and Why Is It Useful?
→ Explain how broadcasting optimizes joins by reducing data shuffling.

2 What Makes Spark Superior to MapReduce?
→ Discuss Spark's in-memory processing and speed advantages over Hadoop MapReduce.

3 How Does Spark Achieve Fault Tolerance?
→ Dive into lineage, RDD immutability, and data recovery.

4 Why Is Caching Crucial in Spark?
→ Share how caching improves performance by storing intermediate results.

5 Explain Broadcast Variables in Spark.
→ Clarify how they minimize data transfer in distributed computations.

## Data Processing & Optimization

6 What Role Does Spark SQL Play in Data Processing?
→ Discuss structured data handling and SQL querying capabilities.

7 How Does Spark Manage Memory Efficiently?
→ Talk about unified memory management and partitioning.

8 Why Is Partitioning Important in Spark?
→ Explain data distribution and its effect on parallelism and performance.

9 Differentiate Between RDDs, DataFrames, and Datasets.
→ Compare their flexibility, performance, and optimization capabilities.

10 How Can You Create a DataFrame in PySpark?
→ Provide examples using spark.read and createDataFrame.

## Advanced PySpark Concepts

11.How Do You Handle Schema Evolution in PySpark?
→ Discuss evolving schemas in structured streaming and data ingestion.

12. Explain Window Functions in PySpark.
→ Describe how to perform operations like ranking and aggregation over partitions.

1️3. What Is a DAG in Spark, and Why Is It Important?
→ Explain how Directed Acyclic Graphs optimize execution.

1️4. Which Version Control Tools Do You Use?
→ Highlight experience with Git, Bitbucket, or similar platforms.

1️5. How Do You Test Your Spark Code?
→ Mention unit testing with PyTest, mocking data, and integration testing.

##Performance & Optimization

1️6. What Is Shuffling, and How Can You Minimize It?
→ Discuss the impact of shuffling and strategies to reduce it partitioning, broadcasting.

1️7. How Does Spark Ensure Fault Tolerance? 
→ Dive deeper into RDD lineage and data recovery.

1️8. Why Is Caching in Spark So Significant? 
→ Reinforce understanding of memory optimization.

1️9. Explain Broadcast Variables in Detail. 
→ Provide real-world use cases for broadcasting.

2️0. How Does Spark SQL Enhance Data Processing? 
→ Discuss schema inference and performance tuning.

21. Early Arriving Facts / Late Arriving Dimensions ? What to do with a Late Arriving Dimension or Early Arriving Facts in your ETL pipeline?

22. Explain difference between SCD and CDC? Which scenarios use them?

23. How Many Tasks Can Run in Parallel in Your Spark Cluster?

24. How Apache Spark Uses CPU, Memory, and Storage





#####Nike Interview questions (Cloudwick) (Hemal and Hasan -HM): 

1) Shuffle partition in spark and explain when it will occur 
2) explain when there is a spark job failure how do you address and what are the steps you will follow 
3) Explan data skewness issue in spark and how do you resolve 
4) Explain when there is a use case given to bring data from source to targe what steps you will follow as a design approach 
5) Difference between emr and databricks and which version you are using?
5) Unity catalog in databricks 
6) ETL vs ELT process which use case fits for which one 
7) How to calculate No of cores,executors, amount of memory in Spark Job?
8) scd2 give me sql logic 
9) What are the challenges associated with running Spark on Databricks and EMR, and how do you address them?
10) EMR instance types and explain ? Which one to use in which scenario?
11) Data bricks delta format and delta tables explain the benfits 
12) Explain any project which you worked on end to end source to target and how much value add to the business
13) While migrating code from EMR to Databricks what challenges you have faced and what are the steps to be taken
14) How do you read hive meta store via databricks unity catalog
15) Explain difference between Rank vs Dense Rank and explain with an example
16) Explain spark submit command and what are the parameters to send and explain them.
–driver-memory
–executor-memory
–num-executors

17) How to Flatten Json Files Dynamically Using Apache Spark

18) Why to avoid multiple chaining of withColumn() function in Spark job.

19) How do you create databricks cluster to run ETL jobs?

20) How do you handle while scheduling airflow job on Day light saving?

21) What is docker and how do you deploy docker container in your pipeline? How you code deploy to Prod?

22) How do you integrate git with databricks for developing and pushing code to Git repo?

23) Can you explain me about semantic layer in databricks with lakehouse and atscale. What are the benefits?

24) What is difference between Delta format vs Parquet format? explain them?

25) How you you config you job parameters for databricks notebook?

27) What KPI's you will consider while building etl pipeline?

28) What kind of datavalidation checks you have done on your source data after it is ingested in Raw layer?

29) Explain Medallion Architecture in your ETL pipeline with Databricks?

33) what is bricksflow in databricks? (CLI tool for development and deployment of python based databricks workflows in a declarative way!(decorators)
  
34) How does great expectations implemented with in databricks for dataquality? advantages of great expectation? (python library for data quality, for validating, documenting and profiling the data - create expectation and validate them)

35)How do you optimize long running etl spark jobs in your databricks?

36) Explain Spark 3 features and its advantages? how did you implement them in your ETL spark scripts?
 Adaptive Query Execution (AQE):  for more intelligent query plans, Dynamic Partition Pruning: to avoid unnecessary data scans.and  cache intermediate results or data frames to avoid re-reading from disk multiple times
 
37) How do you implement coding best practises in your ETL pipeline either with pyspark or spark sql?

38) One sql and python coding easy level to test coding abilities

39) Airflow sensors, operators, and limitations? How do you check source Datafreshness with Airflow?
40) Where is your airflow running either EC2 or MWAA or Astronomer? how they are different to use airflow?
41) How do you create airflow dags dynamically? Explain the process step by step?
42) How would you alert when a airflow job has been failed to the team? What was the process to send alert you have used or you would suggest?

43) How do you integrate databricks with Snowflake and explain your real time experience?
44) Explain RBAC Design and steps in snowflake? Explain Snowflake Stream and tasks and how it is related to CDC? 
45) How do you maintain datawarehouse timestamp etl column on ingested data in your snowflake?
46) How do you mask the PII or PHI data in snowflake dynamically?
47) How to Archive Files in S3 After data COPY INTO Command in Snowflake?

#####Nike Interview questions Databricks focused

https://medium.com/@infinitylearnings1201/databricks-asset-bundle-03-develop-your-first-databricks-asset-bundle-6592cdd347f5

https://medium.com/@vishalbarvaliya/20-real-time-spark-scenario-based-questions-for-data-engineers-2dc893151169

https://medium.com/@infinitylearnings1201/databricks-data-engineering-interview-questions-part-1-expert-answers-01b09dc3e8ba

https://medium.com/@infinitylearnings1201/databricks-data-engineering-interview-questions-part-2-expert-answers-535ff97f6db2

https://medium.com/@infinitylearnings1201/databricks-data-engineering-interview-questions-part-3-expert-answers-7f7e218ab7f5

https://medium.com/@infinitylearnings1201/databricks-data-engineering-interview-questions-part-4-expert-answers-f3a01d7c742e

https://medium.com/@infinitylearnings1201/databricks-data-engineering-interview-questions-expert-level-part-5-2f4724d3d607

1. How does Databricks handle cluster management and resource allocation for optimized performance?
2. What are the best practices for optimizing Apache Spark jobs in Databricks?
3. How does Databricks autoscaling work, and what are the key considerations when enabling it?
4. What are the main performance bottlenecks in Databricks Spark jobs, and how do you troubleshoot them?
5. What are Delta Lakes in Databricks, and how do they improve data reliability compared to Parquet or ORC? What are the key differences between Delta Lake and Parquet?
6. Explain how ACID transactions work in Delta Lake and their benefits in a Data Engineering workflow.
7. How does Databricks handle schema evolution in Delta Lake, and what challenges can arise?
8. What is data skipping in Delta Lake, and how does it optimize query performance?
9. How can you implement data partitioning in Databricks, and when should you repartition data?
10. What are the advantages and disadvantages of using caching in Databricks, and when should you use CACHE TABLE vs. persist()?
11. How does Databricks handle data serialization, and which formats (Parquet, Avro, ORC) are best for different use cases?
12. How can Z-Ordering improve query performance in Delta Lake tables?
13. How can you implement Role-Based Access Control (RBAC) in Databricks for better security?
14. How does Unity Catalog in Databricks enhance data governance and security?
15. What are the best practices for managing secrets and credentials securely in Databricks?
16. What are the best practices for handling bad records in Databricks?
17. Explain the use of Databricks Workflows and their benefits.
18. What do you know about ATScale Semantic layer in Databricks? Explain the benefits and it's usefull to the business and reporting?



####SQL & Python  Coding in Nike Teams:

Write a query to find the second highest salary from an employee table.
How would you find the missing numbers in a given list of numbers from 1 to N?
Write a query to calculate the running total of a sales table by month.
How would you retrieve all rows where a column contains duplicates, but only showing the duplicates once?
Write a query to find the most frequent order status in an orders table.
How would you find the top 3 products with the highest total sales in the last year?
Write a query to calculate the percentage of users who logged in today compared to the total user base.
How would you find all customers who placed an order more than once in the past month?
Write a query to retrieve the employee(s) with the highest salary for each department.
How do you find the first and last purchase date for each customer from an order table?

Write a query to find the first non-null value in a column across multiple rows.
How would you handle performance issues when querying large datasets in SQL?
Write a query to get the average order value per customer, excluding those who have only placed one order.
How would you retrieve the employees who have worked for more than 10 years without using DATEDIFF?
Write a query to find the day of the week that had the maximum number of orders.
How would you retrieve the customer(s) with the longest order history?
Write a query to detect and list duplicate rows in a table based on specific columns.
How would you find the total number of products sold each month, grouped by product category?
Write a query to compare the current year’s sales data against the same period in the previous year.
How would you optimize a slow-running query in SQL involving multiple joins on large tables?


https://www.analyticsvidhya.com/articles/python-coding-interview-questions/

https://medium.com/@nikitasilaparasetty/python-interview-coding-questions-with-solutions-for-beginners-7f6d782defac


Nike Data Modelling:

Mastering Data Modeling: A Complete Guide for Beginners and Beyond:

https://medium.com/itversity/mastering-data-modeling-a-complete-guide-for-beginners-and-beyond-b7a11ef05c0a




###########################################################################################################################################################
###########################################################################################################################################################
###########################################################################################################################################################
Nike Data modelling SQL coding (DEB Peer Team):


Scenario 1: Suppose you have a source data coming from API's and how do you build an ETL or ELT pipeline and which toold you use for the process ?Explain E2E steps with a design diagram and which teams will involve on the data to be delivered to business?


Scenario 2: Suppose you have a source data coming from RDBMS or Flat Files and how do you build an ETL pipeline and which toold you use for the process ?Explain E2E steps with a design diagram and which teams will involve  on the data to be delivered to business?

In above tow scenario how do you build ER diagram and explain the relationships?

Scenario 3: How to Create a Robust ETL Data Pipeline with Python and AWS Services when Source data is either RDBMS or API's . Draw a Datamodel diagram and and derive its facts and dimenstions. (Note: U need to use only AWS services)

SQL coding:

Find the second highest salary without using LIMIT, OFFSET, or TOP.
Given a table of orders, write a query to find the running total (cumulative sum) of revenue for each day.
Write an SQL query to identify employees who earn more than their managers.
Find the top N customers who made the highest purchases, ensuring no duplicates if customers have the same purchase amount.
Identify consecutive login streaks for users where they logged in for at least three consecutive days.
Write an efficient query to detect duplicate records in a table and delete only the extra duplicates, keeping one copy.

Find employees who have the same salary as another employee in the same department.
Write an SQL query to retrieve the department with the highest total salary paid to employees.
Use a window function to rank orders based on order value for each customer, and return only the top 3 orders per customer.
Find the median salary of employees using SQL (without using built-in median functions).
Write a query to pivot a table where each row represents a sales transaction and transform it into a summary format where each column represents a month.
Find the moving average of sales for the last 7 days for each product in a sales table.
Given an events table, find the first and last occurrence of each event per user.
Identify users who have placed an order in two consecutive months but not in the third month.
Find the most frequently purchased product category by each user over the past year.
Write a query to generate a sequential ranking of products based on total sales, but reset the ranking for each year.

How would you write a query to calculate a cumulative sum or running total within a specific partition in SQL?
How do window functions differ from aggregate functions, and when would you use them?
How do you identify and remove duplicate records in SQL without using temporary tables?
Write a query to find the median salary of employees in a table.
Identify and remove duplicate records from a table, keeping the most recent record based on a timestamp column.
Write a query to compute the moving average of daily transactions over a 7-day window.

How would you use window functions to calculate moving averages?
Write a SQL query to calculate the cumulative sum of a column using a window function?
How would you find the top N records for each group using window functions?

How would you find the total number of products sold each month, grouped by product category?
Write a query to compare the current year’s sales data against the same period in the previous year.


https://github.com/keineahnung2345/leetcode-cpp-practices/blob/master/181.%20Employees%20Earning%20More%20Than%20Their%20Managers.sql
###########################################################################################################################################################
###########################################################################################################################################################
##########################################################################################################################################################

#####Nike Chandra Team questions:

How does Databricks handle cluster management and resource allocation for optimized performance?
What are the best practices for optimizing Apache Spark jobs in Databricks?
How does Databricks autoscaling work, and what are the key considerations when enabling it?
What are the main performance bottlenecks in Databricks Spark jobs, and how do you troubleshoot them?
What are Delta Lakes in Databricks, and how do they improve data reliability compared to Parquet or ORC? What are the key differences between Delta Lake and Parquet?
Explain how ACID transactions work in Delta Lake and their benefits in a Data Engineering workflow.
How does Databricks handle schema evolution in Delta Lake, and what challenges can arise?
What is data skipping in Delta Lake, and how does it optimize query performance?
Explain Medallion Architecture in your ETL pipeline with Databricks?
How can you implement data partitioning in Databricks, and when should you repartition data?
What are the advantages and disadvantages of using caching in Databricks, and when should you use CACHE TABLE vs. persist()?
How does Databricks handle data serialization, and which formats (Parquet, Avro, ORC) are best for different use cases?
How can Z-Ordering improve query performance in Delta Lake tables?
How can you implement Role-Based Access Control (RBAC) in Databricks for better security?
How does Unity Catalog in Databricks enhance data governance and security?
What are the best practices for managing secrets and credentials securely in Databricks?
What are the best practices for handling bad records in Databricks?
Explain the use of Databricks Workflows and their benefits.
How do you define dataquality rules with in your ETL pipeline? what steps you have taken to implement them?
Explain greatexpectation framework and how it is usefull to build dataquality in your pipeline?
What do you know about ATScale Semantic layer in Databricks? Explain the benefits and it's usefull to the business and reporting?
Explain Different Environments to Use Apache Spark? Explain using those Environments advantages?
What are the common strategies for tuning Spark jobs to improve performance?
Explain the role of spark.sql.shuffle.partitions and how it affects the performance of Spark SQL queries.
How do you optimize join operations in Spark, particularly when dealing with large datasets?
What are the implications of using wide transformations in Spark? How can you minimize the performance impact of shuffles?
How does Spark’s memory management model influence the performance of a Spark application?
Explain the concept of speculative execution in Spark. When would you enable it, and what are the benefits?
How can you reduce the impact of garbage collection (GC) on Spark performance?
What are the best practices for setting the number of partitions in a Spark job?
Explain the significance of the broadcast join in Spark SQL. How do you determine when to use it?
How do you profile and diagnose performance issues in Spark applications? What tools and techniques do you use?
Explain the concept of Data Locality in Spark. How does it impact job execution?
What is the role of checkpointing in Spark? How is it different from caching?
What are the implications of using Dynamic Resource Allocation in Spark? How does it help in optimizing resource usage?
Discuss the use of the Delta Lake with Apache Spark. How does it improve data reliability and consistency in big data pipelines?
Describe a complex Spark job you have implemented. What challenges did you face, and how did you overcome them?
How would you design a real-time data processing pipeline using Spark?
Explain how you would optimize a Spark job that is suffering from data skew?
How do you manage large-scale Spark jobs that need to run on a daily schedule?
Discuss a scenario where you had to integrate Spark with other big data tools like Kafka, HBase, or Cassandra.
How would you approach migrating a legacy ETL process to a Spark-based solution?
What strategies would you use to handle late-arriving data in a real-time analytics application built on Spark?
How would you design a Spark application to process IoT data at scale?
What considerations would you make when running Spark jobs on a cloud platform like AWS EMR or Databricks?
Discuss a scenario where Spark’s native capabilities were insufficient, and you had to extend Spark with custom code or third-party libraries?
How do you define data quality on your Raw data and what kind of validation rules you have used and applied?
Explain SCD1,SCD2 & SCD3 types in Datawarehouse. Which one used when developing in your ETL piepline?
Explain SCD2 sql logic? How do you implement?
Explain Windows Analytical functions in hive by taking an example dataset.
What is CI/CD process you are following? Explain the steps to deploy code to prod environment.
What is YAML file and how it is used to deploy CI/CD process?
What was your experience with Airflow buling ETL pipelines? Explain different operators you used in your pipeline?
What was your experience working with snowflake? How it is different from data lakehouse?

SQL coding:

Given a table of orders, write a query to find the running total (cumulative sum) of revenue for each day.
Write an SQL query to identify employees who earn more than their managers.
Find the top N customers who made the highest purchases, ensuring no duplicates if customers have the same purchase amount.
Identify consecutive login streaks for users where they logged in for at least three consecutive days.
Write an efficient query to detect duplicate records in a table and delete only the extra duplicates, keeping one copy.
Write a query to find the first and last sales amount for each customer using window functions.
Find employees who have the same salary as another employee in the same department.
Write an SQL query to retrieve the department with the highest total salary paid to employees.
Use a window function to rank orders based on order value for each customer, and return only the top 3 orders per customer.
Find the median salary of employees using SQL (without using built-in median functions).
Write a query to pivot a table where each row represents a sales transaction and transform it into a summary format where each column represents a month.
Find the moving average of sales for the last 7 days for each product in a sales table.
Find the most frequently purchased product category by each user over the past year.
Write a query to generate a sequential ranking of products based on total sales, but reset the ranking for each year.
How would you write a query to calculate a cumulative sum or running total within a specific partition in SQL?
How do window functions differ from aggregate functions, and when would you use them?
How do you identify and remove duplicate records in SQL without using temporary tables?
Write a query to find the moving average of sales for each month, where the moving average window is dynamically adjusted to include the current and the previous two months?
Write a query to compute the moving average of daily transactions over a 7-day window.
How would you use window functions to calculate moving averages?
Write a SQL query to calculate the cumulative sum of a column using a window function?
How would you find the top N records for each group using window functions?
How would you find the total number of products sold each month, grouped by product category?
Write a query to compare the current year’s sales data against the same period in the previous year.

################################


###Cloudwick Client questions (No Coding):


Explain you last 2 project and what was your experience working with SNowflake, Databricks and Airflow?

Explain dynamic masking of data in snowflake? How do you apply row level and column level masking?

What are the type of caching available in Snowflake, How would you identify if your query is returning result from Query Cache/ Metadata cache or VW cache

Can you explain, when would you prefer Transient tables over Permanent Table in Snowflake

What is the role of virtual warehouses in Snowflake, and how do they affect performance?

What are the size range of Micro Partition in Snowflake? How Snowflake processes queries using micro-partitions

How would you choose the perfect size of Virtual warehouse for your requirement

How can we Optimize the Storage & Compute cost in Snowflake

How would you implement SCD type 2 in Snowflake

My client is giving me files in date wise folder in AWS S3 bucket, You need to implement this COPY activity in such a way that it should pick the file from current date folder and load in Snowflake

I have one Parquet file in AWS s3 bucket, Now create table for that file without checking the Stage files

How do you store AWS , Snowflake, databricks secrets while running Airflow dags ?

What is the use of .env file in projects?How to store sensitive information like API keys in .env file?

What are some common performance issues in Snowflake, and how can they be addressed?

How does Snowflake handle failover and disaster recovery?

What are the best practices for handling bad records in Snowflake?

What are Snowflake streams and tasks, and how are they used for data processing?

How does Databricks handle cluster management and resource allocation for optimized performance?

Difference between Databricks Cache vs Apache Spark Cache

What are the best practices for managing secrets and credentials securely in Databricks?

How does Databricks autoscaling work, and what are the key considerations when enabling it?

What are the main performance bottlenecks in Databricks Spark jobs, and how do you troubleshoot them?

What was your approach for migrating snowflake scripts to Databricks Pyspark script? How would you design end to end process in migration? Explain the setup process?

What is an Operator in Airflow, and what are some common types you have used?

How do you make the module available to airflow if you're using Docker Compose?

How do you schedule a DAG to run at a specific interval in Airflow?

How can you trigger DAGs in Airflow, and what are the different ways to do so?

How to control the parallelism or concurrency of tasks in Apache Airflow configuration?

What if your Apache Airflow DAG failed for the last ten days, and now you want to backfill those last ten days' data, but you don't need to run all the tasks of the dag to backfill the data?

What will happen if you set 'catchup=False' in the dag and 'latest_only = True' for some of the dag tasks?

How would you approach if you wanted to queue up multiple dags with order dependencies?

How would you alert when a airflow job has been failed to the team? What was the process to send alert you have used or you would suggest?



***************************
*********AMERI HEALTH**************
***************************
1. What is Databricks and what are its main components? 
2. Explain the concept of a Databricks notebook. 
3. How does Databricks integrate with Apache Spark? 
4. What is Delta Lake and how does it differ from traditional data lakes? 
5. Describe the process of creating and managing clusters in Databricks. 
6. How do you optimize Spark jobs in Databricks for better performance? 
7. Explain the difference between Databricks Runtime and Apache Spark. 
8. What is Unity Catalog in Databricks and how does it enhance data governance? 
9. How do you implement data quality checks in Databricks pipelines? 
10. Describe the process of scheduling and monitoring jobs in Databricks. 
11. What are Databricks widgets and how are they used? 
12. How do you handle streaming data in Databricks? 
13. Explain the concept of Delta Live Tables. 
14. How do you implement version control for Databricks notebooks? 
15. What is the difference between Databricks Community Edition and Enterprise Edition? 
16. How do you integrate Databricks with external data sources? 
17. Explain the use of MLflow in Databricks for machine learning workflows. 
18. How do you implement data security and access control in Databricks? 
19. What is Databricks SQL and how does it differ from traditional SQL engines?
20. How do you optimize Delta Lake tables for query performance?
21. Explain the concept of Auto Loader in Databricks. 
22. How do you handle schema evolution in Delta Lake? 
23. What are the best practices for organizing workspaces in Databricks? 
24. How do you implement CI/CD pipelines for Databricks projects? 
25. Explain the use of Databricks Connect for local development.
26. How do you handle data lineage and auditing in Databricks? 
27. What is Photon engine in Databricks and how does it improve query performance? 
28. How do you implement incremental ETL processes in Databricks? 
29. Explain the concept of Databricks Repos and how it facilitates collaboration. 
30. How do you optimize costs in Databricks while maintaining performance?
31. Explain Databricks VS EMR for spark ETL jobs?
32. Databricks vs snowflake vs Redshift explain whic you will choose to build efficient datawarehouse pipeline ?
33. Medallion Architecture vs Lambda Architecture
34. How do you orchestrate Databricks jobs and notebooks?(Airflow vs databricks jobs)
35. How do you maintain data quality checks on your data? What kind of DQ checks has been implemented?
36. Explain Delta tables and it's storage options. 
37. Explain Timetravel and versioning in Delta Tables.
38. Difference between Delta format and Parquet Format.
39. Explain Vaccum in Databricks?
40. Explain Datalakehouse Semantic layer with Atscale in Databricks?
41. What is Databricks SQL?
42. Any idea about AI like ChatGPT & Copilot and how they are developed? Do you have any interest in working in AI/ML if given in a chance?

***************************
*********NOVANT HEALTH**************
***************************
𝗦𝗤𝗟
- How would you write a query to calculate a cumulative sum or running total within a specific partition in SQL?
- How do window functions differ from aggregate functions, and when would you use them?
- How do you identify and remove duplicate records in SQL without using temporary tables?

𝗣𝘆𝘁𝗵𝗼𝗻
- How do you manage memory efficiently when processing large files in Python?
- What are Python decorators, and how would you use them to optimize reusable code in ETL processes?
- How do you use Python’s built-in logging module to capture detailed error and audit logs?

𝗣𝘆𝘀𝗽𝗮𝗿𝗸
- How would you handle skewed data in a Spark job to prevent performance issues?
- What is the difference between the Spark Session and Spark Context? When should each be used?
- How do you handle backpressure in Spark Streaming applications to manage load effectively?

𝗔𝘇𝘂𝗿𝗲 𝗗𝗮𝘁𝗮𝗯𝗿𝗶𝗰𝗸𝘀
- How do you configure cluster autoscaling in Databricks, and when should it be used?
- How do you implement data versioning in Delta Lake tables within Databricks?
- How would you monitor and optimize Databricks job performance metrics?

𝗔𝘇𝘂𝗿𝗲 𝗗𝗮𝘁𝗮 𝗙𝗮𝗰𝘁𝗼𝗿𝘆
- What are tumbling window triggers in Azure Data Factory, and how do you configure them?
- How would you enable managed identity-based authentication for linked services in ADF?
- How do you create custom activity logs in ADF for monitoring data pipeline execution?

𝗖𝗜/𝗖𝗗
- What are blue-green deployments, and how would you use them for ETL jobs?
- How do you implement rollback mechanisms in CI/CD pipelines for data integration processes?
- What strategies do you use to handle schema evolution in data pipelines as part of CI/CD?

𝗗𝗮𝘁𝗮 𝗪𝗮𝗿𝗲𝗵𝗼𝘂𝘀𝗶𝗻𝗴
- How do you optimize join operations in a data warehouse to improve query performance?
- What is a slowly changing dimension (SCD), and what are different ways to implement it in a data warehouse?
- How do surrogate keys benefit data warehouse design over natural keys?

𝗗𝗮𝘁𝗮 𝗠𝗼𝗱𝗲𝗹𝗶𝗻𝗴
- How do you decide between a star schema and a snowflake schema for a data warehouse? Provide examples of scenarios where each is ideal.
- What is dimensional modeling, and how does it differ from entity-relationship modeling in terms of use cases?
- How do you handle one-to-many relationships in a dimensional model to ensure efficient querying?




#########Tiger Analytics | Data Engineer Interview

Round 1: Technical

This round focused on my experience, projects, and core data engineering concepts related to AWS, Glue, Redshift, and Spark.

*Introduction & Projects

Brief introduction about myself and recent projects.

In-depth questions related to my project architecture and implementation.


*AWS & Data Engineering Concepts

How to connect multiple tables from different AWS databases (RDS, Redshift) using a single connection in AWS Glue.

Different types of triggers in AWS Glue and AWS Step Functions.

Deployment process from DEV to QA to PROD using AWS services.

Creating a CI/CD pipeline using AWS CodePipeline, CodeCommit, and CodeBuild.

Transformations performed in AWS Glue for ETL processing.

Replacing spaces in column names with underscores in source files using AWS Glue and S3.

Understanding SCD Type 2 and how to implement it using AWS Glue or Redshift.

Differences between AWS S3 vs. AWS Redshift in terms of storage, performance, and usage.

Reading data from S3 using Amazon Redshift Spectrum or Athena.


*Coding Questions

Python: Merge two sorted lists into a single sorted list.

SQL: Fetch the 2nd highest salary department-wise and discuss multiple approaches.


Round 2: Technical 

This round was more focused on AWS Glue, Redshift, IAM security, Spark optimizations, and my role in the team.


How to create a view in AWS Glue or Amazon Redshift.

Writing a DDL command in Redshift to create a table.

AWS Glue activities used in my projects.

AWS S3 and IAM security – How to secure access to data in S3.

Authentication methods available in AWS Glue for accessing S3 or RDS.

How does Spark integrate with Hive? Explain how Spark can use Hive metastore and execute Hive queries?

What are the implications of using Dynamic Resource Allocation in Spark? How does it help in optimizing resource usage?

Discuss the use of the Delta Lake with Apache Spark. How does it improve data reliability and consistency in big data pipelines?

What are the challenges associated with running Spark on Kubernetes, and how do you address them?


*Project & Team Discussion

Team size and my role in the project.

My skillset, roles, and responsibilities in the data engineering project (Spark, AWS, ETL, Pipelines).


*ETL & Data Pipeline Design

Designing an ETL pipeline to ingest, transform, and load large datasets from S3 into Amazon Redshift using Spark.

Implementing data versioning in Spark to track dataset changes across different versions.

Spark performance optimizations – when and how to use them.







Product Sense Interview:

https://www.tryexponent.com/blog/product-sense-interview

star method interview:
https://www.betterup.com/blog/star-interview-method