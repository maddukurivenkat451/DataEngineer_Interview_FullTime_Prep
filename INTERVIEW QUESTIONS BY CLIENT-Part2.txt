############################################
****** INTERVIEW QUESTIONS BY CLIENT-Part2 **:
#############################################

******Data engineering Interview questions: Accenture******


Q1.Which Integration Runtime (IR) should be used for copying data from an on-premise database to Azure? 

Q2.Explain the differences between a Scheduled Trigger and a Tumbling Window Trigger in Azure Data Factory. When would you use each?

Q3. What is Azure Data Factory (ADF), and how does it enable ETL and ELT processes in a cloud environment? 

Q4.Describe Azure Data Lake and its role in a data architecture. How does it differ from Azure Blob Storage? 

Q5. What is an index in a database table? Discuss different types of indexes and their impact on query performance. 

Q6.Given two datasets, explain how the number of records will vary for each type of join (Inner Join, Left Join, Right Join, Full Outer Join).

Q7.What are the Control Flow activities in the Azure Data Factory? Explain how they differ from Data Flow activities and their typical use cases.

Q8. Discuss key concepts in data modeling, including normalization and denormalization. How do security concerns influence your choice of Synapse table types in a given scenario? Provide an example of a scenario-based ADF pipeline.

Q9. What are the different types of Integration Runtimes (IR) in Azure Data Factory? Discuss their use cases and limitations.

Q10.How can you mask sensitive data in the Azure SQL Database? What are the different masking techniques available?

Q11.What is Azure Integration Runtime (IR), and how does it support data movement across different networks?

Q12.Explain Slowly Changing Dimension (SCD) Type 1 in a data warehouse. How does it differ from SCD Type 2?

Q13.SQL questions on window functions - rolling sum and lag/lead based. How do window functions differ from traditional aggregate functions?



******American Express (Medium Level) SQL Interview Question — In Detailed Solution:******

https://towardsaws.com/american-express-sql-medium-level-interview-question-in-detailed-solution-171f64127f41


************Amazon************


Here is the interview process had 4 rounds, each focused on different aspects of data engineering.

Round 1: data modeling & sql

the round started with a quick introduction and questions around my experience with data modeling and sql. here are some of the questions:

1.⁠ ⁠write a sql query to fetch the top 3 highest-earning employees from each department. 
2.⁠ ⁠how would you design a data model for an e-commerce application (customers, products, orders)? 
3.⁠ ⁠explain normalization and denormalization—when would you use each? 
4.⁠ ⁠what is a composite primary key, and in which scenario would you use it? 
5.⁠ ⁠how do you handle performance tuning in sql queries? 
6.⁠ ⁠how would you implement slowly changing dimensions (scds) in a data warehouse?

Round 2: big data & distributed systems

this round dug deeper into my knowledge of big data technologies and scalable systems:

1.⁠ ⁠how would you design a data pipeline to process 1 tb of data daily in real-time? 
2.⁠ ⁠explain the differences between hadoop, spark, and flink. which one would you choose for real-time data processing and why? 
3.⁠ ⁠how do you optimize data storage for large-scale datasets on aws s3?
4.⁠ ⁠explain partitioning in hive and how it improves query performance. 
5.⁠ ⁠how would you process a huge dataset using aws glue or emr?

Round 3: aws & cloud services

this round focused heavily on aws services and how they can be used in data engineering:

1.⁠ ⁠describe your experience with aws redshift and how you optimized query performance. 
2.⁠ ⁠how would you architect a scalable etl pipeline using aws lambda and step functions? 
3.⁠ ⁠explain how you would handle security and data governance in an aws data lake setup. 
4.⁠ ⁠discuss your experience with aws glue, redshift, and s3. what are the best practices for optimizing storage and retrieval? 
5.⁠ ⁠how would you implement real-time data ingestion and processing using kinesis or kafka?
even though i didn’t make it through all the rounds, it was a valuable learning experience.
⭐ personal advice: amazon’s interviews, especially for data engineers, are very scenario-based and focused on real-world applications. i realized that i could have done better if i practiced more on distributed systems and aws architecture.
so, if you’re looking for guidance throughout your prep, consider getting help from bosscoder academy to receive mentorship from industry professionals.
check them here: https://bit.ly/3xu18tj 

enrol in their program and get:
✅ structured curriculum to master etl & warehousing, big data & cloud, advanced data ops, and more.
✅ personalized guidance from experts working at google, samsung, and other top companies.
✅ multiple projects focused on big data pipeline, data processing and other in-demand skills to build a strong portfolio.